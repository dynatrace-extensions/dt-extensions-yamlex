import json
import logging
import re
from pathlib import Path
from typing import Union


logger = logging.getLogger(__name__)


def map_schema_to_sources(
    schema: Path,
    sources: Path,
    root: Path,
    extension_yaml: Path,
):
    mapping: dict[str, Union[str, list]] = dict()

    # Make sure the directories are relative to the project directory
    root = root.resolve()
    schema = schema.resolve().relative_to(root)
    sources = sources.resolve().relative_to(root)

    # ------------------------------
    # Generated extension assembly
    # ------------------------------

    mapping[(schema / "extension.schema.json").as_posix()] = [
        extension_yaml.as_posix(),
        (sources / "index.yaml").as_posix(),
    ]

    # ------------------------------
    # Metrics
    # ------------------------------

    mapping[(schema / "extension.metrics.schema.json").as_posix()] = [
        (sources / "metrics" / "*.yaml").as_posix(),
        (sources / "metrics" / "+*" / "*.yaml").as_posix(),
    ]

    # ------------------------------
    # Datasources
    # ------------------------------

    # gcp
    mapping[(schema / "gcp.service.schema.json").as_posix()] = [
        (sources / "gcp" / "*.yaml").as_posix(),
    ]

    # jmx
    mapping[(schema / "jmx.group.schema.json").as_posix()] = [
        (sources / "jmx" / "*.yaml").as_posix(),
    ]

    # processes
    mapping[(schema / "processes.object.schema.json").as_posix()] = [
        (sources / "processes" / "*.yaml").as_posix(),
    ]

    # prometheus
    mapping[(schema / "prometheus.object.schema.json").as_posix()] = [
        (sources / "prometheus" / "*.yaml").as_posix(),
    ]

    # snmp
    mapping[(schema / "snmp.object.schema.json").as_posix()] = [
        (sources / "snmp" / "*.yaml").as_posix(),
    ]

    # snmptraps
    mapping[(schema / "snmptraps.object.schema.json").as_posix()] = [
        (sources / "snmptraps" / "*.yaml").as_posix(),
    ]

    # sql
    mapping[(schema / "sql.object.schema.json").as_posix()] = [
        (sources / "sqlDb2" / "*.yaml").as_posix(),
        (sources / "sqlHana" / "*.yaml").as_posix(),
        (sources / "sqlMySql" / "*.yaml").as_posix(),
        (sources / "sqlPostgres" / "*.yaml").as_posix(),
        (sources / "sqlOracle" / "*.yaml").as_posix(),
        (sources / "sqlServer" / "*.yaml").as_posix(),
        (sources / "sqlSnowflake" / "*.yaml").as_posix(),
    ]

    # wmi
    mapping[(schema / "wmi.object.schema.json").as_posix()] = [
        (sources / "wmi" / "*.yaml").as_posix(),
    ]

    # ------------------------------
    # Topology
    # ------------------------------

    mapping[(schema / "generic.types.object.schema.json").as_posix()] = [
        (sources / "topology" / "types" / "*.yaml").as_posix(),
    ]

    mapping[(schema / "generic.relationships.object.schema.json").as_posix()] = [
        (sources / "topology" / "relationships" / "*.yaml").as_posix(),
    ]
    
    # ------------------------------
    # Screens
    # ------------------------------

    mapping[(schema / "extension.screens.schema.json").as_posix()] = [
        (sources / "screens" / "*.yaml").as_posix(),
        (sources / "screens" / "*" / "index.yaml").as_posix(),
    ]

    # Actions
    mapping[(schema / "screen.action.schema.json").as_posix()] = [
        (sources / "screens" / "*" / "actions.yaml").as_posix(),
    ]
    mapping[(schema / "screen.actions.object.schema.json").as_posix()] = [
        (sources / "screens" / "*" / "actions" / "index.yaml").as_posix(),
        (sources / "screens" / "*" / "actions" / "-*.yaml").as_posix(),
    ]

    return mapping


def validate_json_schemas_dir(json_schemas_dir_path: Path):
    """Make sure that source directory with JSON schema contains valid JSON schema files"""
    if not (json_schemas_dir_path / "extension.schema.json").exists():
        logger.error(f"{json_schemas_dir_path} directory does not contain extension.schema.json file.")
        exit(1)


def extract_definitions_into_standalone_schemas(json_schemas_dir_path: Path) -> None:
    """Open each schema file and extract definitions into standalone schema files"""
    # Get a list of all JSON schema files in the directory
    schema_file_paths = list(json_schemas_dir_path.glob("*.json"))
    for schema_file_path in schema_file_paths:
        relative_schema_file_path = schema_file_path.relative_to(json_schemas_dir_path)
        # Get stem two times to convert extension.schema.json to extension
        parent_schema_stem = Path(schema_file_path.stem).stem 
        logger.debug(f"Processing schema file: {relative_schema_file_path}")

        with open(schema_file_path, "r") as f:
            schema_json_text = f.read()
            schema = json.loads(schema_json_text)

            # Do not process already extracted schemas.
            # Schema files generated by us do not start with http
            if not str(schema.get("$id")).startswith("http"):
                # Skip
                continue

            # Collect potential new schemas to process
            new_schemas_to_process: dict[str, dict] = {}

            if "definitions" in schema:
                for d, definition in schema["definitions"].items():
                    # Check if the definition is nested
                    if "enums" == d:
                        logger.debug(f"Nested enums found in {relative_schema_file_path}.")
                        for enum_name, enum_def in definition.items():
                            logger.debug(f"Extracting enum: {enum_name}")
                            new_schemas_to_process[f"enums/{enum_name}"] = enum_def
                    elif "types" == d:
                        for type_name, type_def in definition.items():
                            logger.debug(f"Extracting type: {type_name}")
                            new_schemas_to_process[f"types/{type_name}"] = type_def
                    elif (
                        definition.get("type") == "array"
                        and "items" in definition
                        and "$ref" not in definition["items"]
                    ):
                        logger.debug(f"Extracting array item definition: {d}")
                        new_schemas_to_process[d] = definition["items"]
                    else:
                        logger.debug(f"Extracting definition: {d}")
                        new_schemas_to_process[d] = definition

            if "properties" in schema:
                for p, property in schema["properties"].items():
                    if (
                        property.get("type") == "array"
                        and "items" in property
                        and "$ref" not in property["items"]
                    ):
                        logger.debug(f"Extracting array item property: {p}")
                        new_schemas_to_process[p] = property["items"]

            if (
                schema.get("type") == "array"
                and "items" in schema
                and "$ref" not in schema["items"]
            ):
                logger.debug(f"Extracting child item from array: {relative_schema_file_path}")
                new_schemas_to_process["object"] = schema["items"]

            for name, new_schema in new_schemas_to_process.items():
                valid_name = name.replace("/", ".")
                extract_single_definition_into_standalone_schema(
                    parent_schema_stem,
                    valid_name,
                    json_schemas_dir_path,
                    new_schema,
                )


def extract_single_definition_into_standalone_schema(
    parent_schema_stem: str,
    name: str,
    json_schemas_dir_path: Path,
    definition: dict,
):
    file_name = f"{parent_schema_stem}.{name}.schema.json"
    enrich_definition_for_standalone_schema(definition, file_name)

    # Write the enriched definition to a new schema file
    d_schema_file_path = json_schemas_dir_path / file_name
    with open(d_schema_file_path, "w") as d_schema_file:
        d_schema_text = json.dumps(definition, indent=2)

        # If extracted definition contains references to sibling definitions,
        # then replace them with new extracted schema file names
        # Regex to replace "#/definitions/{name}"" reference with "{name}.schema.json"

        # Find all references to sibling definitions using regex
        for m in re.finditer(r'"#/definitions/(.+)"', d_schema_text):
            full_match = m.group(0)
            logger.debug(f"Found reference to sibling definition: {full_match}")
            catch_replacement = m.group(1).replace("/", ".")
            full_replacement = f'"{parent_schema_stem}.{catch_replacement}.schema.json"'
            logger.debug(f'Will be replaced with: {full_replacement}')
            d_schema_text = d_schema_text.replace(full_match, full_replacement)

        # Save to file
        d_schema_file.write(d_schema_text)
        logger.info(f"Extracted new schema: {file_name}")


def enrich_definition_for_standalone_schema(definition: dict, file_name: str):
    definition["$schema"] = "http://json-schema.org/draft-07/schema#"
    definition["$id"] = file_name
    definition["description"] = f"Representation of {file_name}"
    # definition["additionalProperties"] = False


def read_vscode_settings_json_file(vscode_settings_json_file_path: Path):
    """Read settings.json if it exists, otherwise create an empty dictionary."""
    if vscode_settings_json_file_path.exists():
        with open(vscode_settings_json_file_path, "r") as f:
            vscode_settings_file_text = f.read()
            vscode_settings = json.loads(vscode_settings_file_text)
    else:
        vscode_settings = {}
    return vscode_settings
